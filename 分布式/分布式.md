# 集群

集群调度架构 

[(3条消息) 十大主流集群调度系统大盘点_不正常IT研究所的博客-CSDN博客](https://blog.csdn.net/vip_iter/article/details/80123228)



# 分布式

​	微服务: 项目的一种架构方式(概念),如mvc架构

​	sc 则是这种架构的一种技术实现

​	同时微服务则一定需要分布式解决方案



与集群的区别:

-  分布式是指将不同的业务分布在不同的地方

- 集群指的是将几台服务器集中在一起，实现同一业务
- 分布式的每一个节点，都可以做集群，而集群并不一定就是分布式的。 而分布式，从狭义上理解，也与集群差不多，但是它的组织比较松散，不像集群，有一定组织性，一台服务器宕了，其他的服务器可以顶上来。分布式的每一个节点，都完成不同的业务，一个节点宕了，这个业务就不可访问了

## 场景

用户- 分布式session

报表-分布式调度

秒杀-分布式限流

订单-分布式id,分库分表,分布式事务



#  分布式id

传统方案: 自增/时间戳 uuid

分布式id特点:

1. 全局唯一
   - 应用只部署在一个服务器上,生成id唯一
   - 应用只部署在一个服务器上,生成id唯一
   - 应用只部署在同一个服务器上不同的docker容器中,生成id唯一
2. 高并发
3. 高可用



方案优缺点:

1. UUID 

   缺点: 1.空间占用多 2.索引效率低

2. 数据库自增组件

   缺点: 1.并发性能不高,受限于数据库性能  2. 针对缺点1 分库分表 需要改造 较复杂  3.业务数据容易泄露

3. redis自增

   优点: 并发性能相对方案2高

   缺点: 1. 数据丢失(rdb至少丢失1s数据,aof可以做到不丢失但地洞回复速度慢) 2.业务数据容易泄露

4. 雪花算法(snowflake)

   优点: 不依赖外部组件(如数据库,redis等)

   缺点: 时钟回拨

5. 号段模式

   缺点: 仍然依赖数据库,且仍然是自增

   优点: 一次获取多个可用id,性能提高



自研算法分析:

时间戳(ms) 同一ms可能有多个请求

时间戳(ms)+随机数  减少重复的概率,但无法完全保证唯一

时间戳(ms)+自增数(AtomicInteger) 同一台机器(容器)保证唯一,但多台机器无法保证

时间戳(ms)+机器序号+自增数(AtomicInteger)



雪花算法就是类似上述

比如:

1bit 符号位(舍弃)

41bit  时间戳

10bit 工作进程位=5bit 区域 + 5bit 机器序号

12bit 序列号位

大小可以自定义







## generator(百度)

雪花算法

https://github.com/baidu/uid-generator/blob/master/README.zh_cn.md









## Tinyid(滴滴)

https://github.com/didi/tinyid

https://github.com/didi/tinyid/wiki

基于数据库号段算法

多DB,高可用,java-client

##  Leaf(美团)

号段模式和snowflake模式

https://github.com/Meituan-Dianping/Leaf/blob/master/README_CN.md

用到zookeeper,程序运行在docker等(手动配置不方便),自动上报服务器标识到zk,zk存储

分布式id以此为例(spring boot)

1. 引入依赖

   ```<dependency>
   <dependency>
   	<artifactId>leaf-boot-starter</artifactId>
       <groupId>com.sankuai.inf.leaf</groupId>
       <version>1.0.1-RELEASE</version>
   </dependency>
   ```

   1.1 若maven仓库没有 需要先下载install

   ```
   git clone git@github.com:Meituan-Dianping/Leaf.git
   或 
   git clone https://github.com/Meituan-Dianping/Leaf.git
   cd leaf
   git checkout feature/spring-boot-starter
   mvn clean install -Dmaven.test.skip=true 
   ```

2. 配置leaf.properties

   ```
   leaf.name=com.sankuai.leaf.opensource.test
   leaf.segment.enable=false
   #leaf.segment.url=
   #leaf.segment.username=
   #leaf.segment.password=
   
   leaf.snowflake.enable=false
   #leaf.snowflake.address=
   #leaf.snowflake.port=
   ```

3. @EnableLeafServer

4.     @Autowired
       private SegmentService segmentService;
       @Autowired
       private SnowflakeService snowflakeService;

5. 表结构(号段模式)

   ```
   CREATE DATABASE leaf;
   use leaf;
   CREATE TABLE `leaf_alloc` (
     `biz_tag` varchar(128)  NOT NULL DEFAULT '',
     `max_id` bigint(20) NOT NULL DEFAULT '1',
     `step` int(11) NOT NULL,
     `description` varchar(256)  DEFAULT NULL,
     `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
     PRIMARY KEY (`biz_tag`)
   ) ENGINE=InnoDB;
   
   insert into leaf_alloc(biz_tag, max_id, step, description) values('leaf-segment-test', 1, 2000, 'Test leaf Segment Mode Get Id')
   ```

   

6. ```
   @RequestMapping("/leaf")
   public class LeafController {
       @Autowired
       private SegmentService segmentService;
   /*    @Autowired
       private SnowflakeService snowflakeService;*/
   
       @GetMapping("/segment")
       public long segment(){
          return segmentService.getId("leaf-segment-test").getId(); //对应leaf-alloc biz_tag字段
       }
   }
   ```



雪花模式需要先搭建zk服务器



问题:

1. java.sql.SQLException: validateConnection false

   版本问题

   比如druid 1.0.18 需要匹配5.1.38的mysql-connector-java

   3.3.1的mybatis

   这里升级druid

   ```
   <dependency>
      <groupId>com.alibaba</groupId>
      <artifactId>druid</artifactId>
      <version>1.2.5</version>
   </dependency>
   <dependency>
   			<groupId>org.mybatis</groupId>
   			<artifactId>mybatis</artifactId>
   			<version>3.4.0</version>
   		</dependency>
   ```

   



# 分布式session

## 分类

### 传统session



![image-20220803135327995](assets/image-20220803135327995.png)



问题一: session存在哪里

​	一般情况下,session都是存储在内存中(服务器(tomcat)中),可以设置session持久化,在服务器重启时能够加载

问题二: nginx做了负载均衡,可能导致登录失效问题,怎么解决?

 1. 做某种规则,让特定用户访问特定服务器,如张三始终访问服务器1

    问题: 破坏了高可用,服务器1挂了,nginx的请求仍会转发到服务器1.

 2. 既然是session存在不同地方导致的,那就把session存在同一地方,如redis



session 原理:



![image-20220803142925996](assets/image-20220803142925996.png)

**浏览器第一次访问服务器的时候,服务器会生成一个sessionid,并返回给浏览器,这个sessionid会保存在浏览器的cookie会话中**

这个机制是根据浏览器中是不是有cookie,没有就重新生成(删除cookie记录后就没有了)

set-Cookie



问题:跨域

http://localhost:8080/ 

http://127.0.0.1:8080/

上述两个代表不同的域,当分别访问时会生成不同的cookie



总结:

1. 访问同一服务器下不同的web容器(如tomcat,可以是同一应用)生成不同的sessionId
2. 以不同域访问一个web容器(localhost,127.0.0.1 ),生成不同的sessionId

### spring-session

将session存到redis

1. redisTemplate
2. spring-session-data-redis

```
<dependency>
   <groupId>org.springframework.session</groupId>
   <artifactId>spring-session-data-redis</artifactId>
</dependency>
```

```
spring.session.store-type=redis
spring.session.timeout=3600
spring.session.redis.namespace=login:user //spring:Session扔存在,生成两份
```

如果redis集群配置的, spring.redis下配置



Spring Session makes it trivial(微不足道) to support clustered sessions without being tied to(be bounded to) an application container specific solution. It also provides transparent (透明)integration with

1. `HttpSession` - allows replacing the HttpSession in an application container (i.e. Tomcat) neutral(中立,非侵入式) way, with support for providing session IDs in headers to work with RESTful APIs

   原来Httpsession怎么用,现在就怎么用  `HttpSessionWrapper`

2. `WebSocket` - provides the ability to keep the HttpSession alive when receiving WebSocket messages

3. `WebSession` - allows replacing the Spring WebFlux’s WebSession in an application container neutral way



源码分析:

1. SessionRepositoryFilter.SessionRepositoryRequestWrapper implements HttpServletRequestWrapper

2. SessionRepositoryFilter.SessionRepositoryRequestWrapper implements HttpServletRequestWrapper
   SessionRepositoryFilter<S extends Session> extends OncePerRequestFilter

3. parent.doFilter -> child.doFilterInternal -> ApplicationFilterChain.doFilter ->Call the next filter if there is one(pos<n)

4. FilterChains

   ```
   0 = {ApplicationFilterConfig@10174} "ApplicationFilterConfig[name=characterEncodingFilter, filterClass=org.springframework.boot.web.servlet.filter.OrderedCharacterEncodingFilter]"
   1 = {ApplicationFilterConfig@10175} "ApplicationFilterConfig[name=sessionRepositoryFilter, filterClass=org.springframework.session.web.http.SessionRepositoryFilter]"
   2 = {ApplicationFilterConfig@10108} "ApplicationFilterConfig[name=formContentFilter, filterClass=org.springframework.boot.web.servlet.filter.OrderedFormContentFilter]"
   3 = {ApplicationFilterConfig@10176} "ApplicationFilterConfig[name=requestContextFilter, filterClass=org.springframework.boot.web.servlet.filter.OrderedRequestContextFilter]"
   4 = {ApplicationFilterConfig@10177} "ApplicationFilterConfig[name=Tomcat WebSocket (JSR356) Filter, filterClass=org.apache.tomcat.websocket.server.WsFilter]"
   ```

   

快捷键: ctrl+alt+左键 弹出实现(类和方法)

#### servlet和interceptor的区别

这两者在功能方面很类似,并且都是通过AOP(一种思想,横向切面,对OOP纵向扩展的补充)的具体技术实现.但在具体技术实现方面，差距还是比较大的.

区别:

- Filter是依赖于Servlet[容器](https://cloud.tencent.com/product/tke?from=10680)，属于Servlet规范的一部分，而拦截器则是独立存在的，可以在任何情况下使用。

- Filter的执行由Servlet容器回调完成，而拦截器通常通过动态代理的方式来执行。

  默认有上述5个

  ```
  @WebFilter(urlPatterns = "/*",filterName="filter3")
  public class MyFilter1 implements Filter {
  
      @Override
      public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException {
      	...
          chain.doFilter(request,response);
          ...
    }
  ```

  ApplicationFilterChain.doFilter 链式  

  ```
  ...
  filter.doFilter(request, response, this);
  ...
  return;
  ```

  先进入doFilter方法 后结束

- Filter的生命周期由Servlet容器管理，而拦截器则可以通过IoC容器来管理，因此可以通过注入等方式来获取其他Bean的实例，因此使用会更方便。

```
@Configuration
public class WebConfigure extends WebMvcConfigurerAdapter {
    @Override
    public void addInterceptors(InterceptorRegistry registry) {
//        registry.addInterceptor(new MyInterceptor1()).addPathPatterns("/**");
    }
}

public class MyInterceptor1 implements HandlerInterceptor {
```

注意: urlPatterns /*  pathPatterns /**





### token+redis

token+redis自由度高(移动端不太喜欢用cookie,以上两种都是基于cookie)

cookie的缺陷

1. 不安全
2. 大小有限,只能存储简单的字符串
3. 能够被禁用,删除



**token推荐存在header里**

@RequestHeader可以获得全部cookie

@CookieValue("SESSION") 从cookie中获取指定key的值

​	Cookie: 

​	JSESSIONID=40D2D611E06D4DB13DCD35F1B5220E0F; SESSION=ZGQzYTIyOTktODk0Yi00NDczLWJiNzktYjNkOWEyZDE5Nzc5;...









### jwt

JSON WEB TOKEN 一种协议

jwt.io



```
<dependency>
    <groupId>com.auth0</groupId>
    <artifactId>java-jwt</artifactId>
    <version>4.0.0</version>
</dependency>
```


#### 实例

Example using `HS256`

1. create

   ```
   Algorithm algorithm = Algorithm.HMAC256("token_secret");
   String token = JWT.create()
           .withClaim("login_user",username
           .withExpiresAt()  //TokenExpiredException
           .sign(algorithm);
   ```

2. verify

   ```
   Algorithm algorithm = Algorithm.HMAC256("token_secret");
   JWTVerifier verifier = JWT.require(algorithm)
           .build(); //Reusable verifier instance
   DecodedJWT jwt = verifier.verify(token); //JWTCreationException JWTDecodeException
   jwt.getClaim("login_user").asString()
   ```

3. DecodedJWT

   组成部分: header,payload,verify signature,token

   token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJsb2dpbl91c2VyIjoiYWRtaW4ifQ.Cq1TvoFJHvNjXhSecW1Kovx1Aymud3GXQEmThAH7evQ

   header=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9

   {
     "typ": "JWT",
     "alg": "HS256"
   }

   

   payload(data)=eyJsb2dpbl91c2VyIjoiYWRtaW4ifQ

   {
     "login_user": "admin"
   }

   

   signature=Cq1TvoFJHvNjXhSecW1Kovx1Aymud3GXQEmThAH7evQ

   ```
   HMACSHA256(
     base64UrlEncode(header) + "." +
     base64UrlEncode(payload),
     your-256-bit-secret
   )
   ```



当传递非之前生成的token 会报错(可解析,但防篡改)

```
com.auth0.jwt.exceptions.SignatureVerificationException: The Token's Signature resulted invalid when verified using the Algorithm: HmacSHA256
```

原因: 在生成jwt的时候Algorithm.HMAC256加入了自定义的`JWT_SECRET`,因此要妥善保存`JWT_SECRET`

#### claims

Claims are issued by(be provided by) a provider, and they are given one or more values and then packaged in security tokens that are issued by an issuer, commonly known as a security token(令牌) service (STS). Claim is **piece of information that describes given identity(身份) on some aspect**. Take claim as name-value pair

## 拦截器统一处理token

```
public class JwtInterceptor implements HandlerInterceptor {
    //如果该类注入了其他组件,如redisTemplate需要将该拦截器注册为组件,在WebConfigure注入
    @Override
    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {
        try {
            String token=request.getHeader("token");
            if(StringUtils.isBlank(token)){
                throw  new RuntimeException("token must be passed in header");
            }
            Algorithm algorithm = Algorithm.HMAC256("token_secret");
            JWTVerifier verifier = JWT.require(algorithm)
                    .build(); //Reusable

            // verifier instance
            DecodedJWT jwt = verifier.verify(token);
            //同一个request,session,redis都能存储
/*            request.getSession().setAttribute("login_user",
                    jwt.getClaim("login_user").asString());*/
            request.setAttribute("login_user",
                    jwt.getClaim("login_user").asString()); //@RequestAttribute
            return true;
        } catch (Exception exception){
            //Invalid signature/claims
            exception.printStackTrace();
        }
        return false;
    }
}
```

jwt.getClaim("login_user").asString() 注意数据类型 如果login_user是int类型,asString()->null



## Oauth2

授权机制,跟JWT完全不一样

比如qq授权慕课网(第三方)获取用户信息



spring-security-oauth2

1. 授权数据给第三方
2. 权限管理





# 分布式任务调度

场景

1. 报表

2. 日/月结单

3. 爬虫

4. 数据归档

   ...

总结: 基于时间+基于数据的任务调度



## 非分布式

@Scheduled

```
@Slf4j
@Component
@EnableScheduling
public class ScheduleJob {

    @SneakyThrows
    @Scheduled(fixedRate = 3000,initialDelay = 4000)
    public void shcedule(){
        log.info("job start....");
        Thread.sleep(2000);
        log.info("job end....");
    }
}
```

### 参数分析

- initialDelay: Number of units of time to delay before the first execution of a fixedRate or fixedDelay task.

- fixedRate:  a fixed period between invocations 上一次调用开始到这次调用开始中间的时间

- fixedDelay:a fixed period between the end of the last invocation and the start of the next 上一次调用结束到这次调用开始的间隔时间

* cron expressions

  https://cron.qqe2.com/

  1. Seconds
  2. Minutes
  3. Hours
  4. Day-of-Month
  5. Month
  6. Day-of-Week
  7. Year (optional field)

  上述自表达式用空格隔开

    “0 0 12？* WED“ - 这意味着”每个星期三下午12:00“

  分钟 0/15 0,15都表示从0开始每隔15分钟

  *每个可能的值（枚举）

  ？表示无特定值的日期和星期几 表达式中只有1个?

  L

  * 月 月末
  * 周 7或SAT
  * 6L 或 FRIL本月最后一个周五

  W 指定最近给定日期的工作日（星期一至星期五） 15W表示最近15日的工作日

  “6＃3”或“FRI＃3” 本月的第3个星期5

* ```
  MISFIRE_INSTRUCTION_FIRE_ONCE_NOW
  MISFIRE_INSTRUCTION_DO_NOTHING
  ```

### 多线程

```
@SneakyThrows
@Scheduled(fixedRate = 3000)
public void shcedule(){
    log.info("job1 start....");
    Thread.sleep(5000);
    log.info("job1 end....");
}

@SneakyThrows
@Scheduled(fixedRate = 3000)
public void shcedule2(){
    log.info("job2 start....");
    Thread.sleep(5000);
    log.info("job2 end....");
}
```

![image-20220808091605730](assets/image-20220808091605730.png)

通过日志可以了解到,只有main线程交替执行各个schedule

如果想并发执行schedule,各个schedule独立执行,可以引入多线程

```
@Bean
public TaskScheduler taskScheduler(){
    ThreadPoolTaskScheduler taskScheduler = new ThreadPoolTaskScheduler();
    taskScheduler.setPoolSize(5);
    return taskScheduler;
}
```

![image-20220808092219247](assets/image-20220808092219247.png)



fixedRate = 3000,如果task执行所需时间>3s,此时两个task开始之间间隔>3s,如果我想task只要到了3s就要重新起一个,可以考虑异步.

1. ```
   @Slf4j
   @Async
   @Component
   public class AsynTaskService {
   
       @SneakyThrows
       public void doAsynProcess(){
           log.info("doProcess1 start ");
           Thread.sleep(5000);
           log.info("doProcess1 end ");
       }
   }
   ```

2. ```
   @Autowired
   private AsynTaskService asynTaskService;
    @Scheduled(fixedRate = 3000)
       public void shcedule(){
           log.info("job1 start....");
           asynTaskService.doAsynProcess();
           log.info("job1 end....");
       }
   
   ```

3. ```
   @EnableAsync
   ```



![image-20220808100900320](assets/image-20220808100900320.png)

job1和doProcess1用不同的线程池管理.





当task急剧增多,单机难以处理,此时可以考虑分布式

## 分布式

1. Quartz

   借助数据库实现分布式

   参照quartz.md

2. Elasticjob

3. XXL-JOB

4. ScheduledX

5. PowerJob

   |                | QuartZ                           | xxl-job                                  | SchedulerX 2.0                                    | **PowerJob**                                                 |
   | -------------- | -------------------------------- | ---------------------------------------- | ------------------------------------------------- | ------------------------------------------------------------ |
   | 定时类型       | CRON                             | CRON                                     | CRON、固定频率、固定延迟、OpenAPI                 | **CRON、固定频率、固定延迟、OpenAPI**                        |
   | 任务类型       | 内置Java                         | 内置Java、GLUE Java、Shell、Python等脚本 | 内置Java、外置Java（FatJar）、Shell、Python等脚本 | **内置Java、外置Java（容器）、Shell、Python等脚本**          |
   | 分布式任务     | 有                               | 静态分片                                 | MapReduce 动态分片                                | **MapReduce 动态分片**                                       |
   | 在线任务治理   | 不支持                           | 支持                                     | 支持                                              | **支持**                                                     |
   | 日志白屏化     | 不支持                           | 支持                                     | 不支持                                            | **支持**                                                     |
   | 调度方式及性能 | 基于数据库锁，有性能瓶颈         | 基于数据库锁，有性能瓶颈                 | 不详                                              | **无锁化设计，性能强劲无上限**                               |
   | 报警监控       | 无                               | 邮件                                     | 短信                                              | **邮件，提供接口允许开发者扩展**                             |
   | 系统依赖       | 关系型数据库（MySQL、Oracle...） | MySQL                                    | 人民币                                            | **任意 Spring Data Jpa支持的关系型数据库（MySQL、Oracle...）** |
   | DAG 工作流     | 不支持                           | 不支持                                   | 支持                                              | **支持**                                                     |

   









## XXL-JOB

![1659958634670](assets/1659958634670.png)

### 配置部署调度中心

1. https://github.com/xuxueli/xxl-job

   http://gitee.com/xuxueli0323/xxl-job

   下载到本地

   mvn clean install -Dmaven.test.skip=true(可选)

   1. `xxl-job-admin：调度中心`
   2. `xxl-job-core：公共依赖`
   3. `xxl-job-executor-samples：执行器Sample示例（选择合适的版本执行器，可直接使用，也可以参考其并将现有项目改造成执行器）`
   4. `    ：xxl-job-executor-sample-springboot：Springboot版本，通过Springboot管理执行器，推荐这种方式；`
   5. `    ：xxl-job-executor-sample-frameless：无框架版本；`

2. 初始化数据库表

   `/xxl-job/doc/db/tables_xxl_job.sql`

3. 配置部署"调度中心"(也就是后台,可通过Docker 部署)

   1. 修改配置`/xxl-job/xxl-job-admin/src/main/resources/application.properties`

   ```
   ### 调度中心JDBC链接：链接地址请保持和 2.1章节 所创建的调度数据库的地址一致
   spring.datasource.url=jdbc:mysql://127.0.0.1:3306/xxl_job?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&serverTimezone=Asia/Shanghai
   spring.datasource.username=root
   spring.datasource.password=root_pwd
   spring.datasource.driver-class-name=com.mysql.jdbc.Driver
   ### 报警邮箱
   spring.mail.host=smtp.qq.com
   spring.mail.port=25
   spring.mail.username=xxx@qq.com
   spring.mail.password=xxx
   spring.mail.properties.mail.smtp.auth=true
   spring.mail.properties.mail.smtp.starttls.enable=true
   spring.mail.properties.mail.smtp.starttls.required=true
   spring.mail.properties.mail.smtp.socketFactory.class=javax.net.ssl.SSLSocketFactory
   ### 调度中心通讯TOKEN [选填]：非空时启用；
   xxl.job.accessToken=
   ### 调度中心国际化配置 [必填]： 默认为 "zh_CN"/中文简体, 可选范围为 "zh_CN"/中文简体, "zh_TC"/中文繁体 and "en"/英文；
   xxl.job.i18n=zh_CN
   ## 调度线程池最大线程配置【必填】
   xxl.job.triggerpool.fast.max=200
   xxl.job.triggerpool.slow.max=100
   ### 调度中心日志表数据保存天数 [必填]：过期日志自动清理；限制大于等于7时生效，否则, 如-1，关闭自动清理功能；
   xxl.job.logretentiondays=30
   ```

   

   2. 将E:\xxl-job\xxl-job-admin打包部署在服务器上

      java -jar  xxl-job-admin-2.4.0-SNAPSHOT.jar

      http://localhost:8080/xxl-job-admin (该地址执行器将会使用到，作为回调地址)

      admin/123456

   3. 调度中心集群(可选)



通过Docker部署

2.3.0

1. docker pull xuxueli/xxl-job-admin:{指定版本} 

2. docker run -p 8090:8090 -v E:\docker_logs:/data/applogs --name xxl-job-admin  -d xuxueli/xxl-job-admin:2.3.0

   | `--detach` , `-d` |      | Run container in background and print container ID           |
   | ----------------- | ---- | ------------------------------------------------------------ |
   |                   |      | **将子线程和主线程的关联分离** ,子线程在后台独立继续运行，主线程无法再取得子线程的控制权，即使主线程结束，子线程未执行也不会结束。 |

   ```
   /**
   * 如需自定义 mysql 等配置，可通过 "-e PARAMS" 指定，参数格式 PARAMS="--key=value  --key2=value2" ；
   * 配置项参考文件：/xxl-job/xxl-job-admin/src/main/resources/application.properties
   * 如需自定义 JVM内存参数 等配置，可通过 "-e JAVA_OPTS" 指定，参数格式 JAVA_OPTS="-Xmx512m" ；
   */
   ```

   docker run -e PARAMS="--spring.datasource.url=jdbc:mysql://127.0.0.1:3306/xxl_job?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&serverTimezone=Asia/Shanghai" -p 8088:8088 -v E:\docker_logs:/data/applogs --name xxl-job-admin  -d xuxueli/xxl-job-admin:2.3.0 

  



问题：21:01:11.874 logback [xxl-job, admin JobFailMonitorHelper] ERROR com.zaxxer.hikari.pool.HikariPool - HikariCP - Exception during pool initialization. java.sql.SQLNonTransientConnectionException: Could not create connection to database server. Attempted reconnect 3 times. Giving up

原因: 没有在docker安装mysql且docker run参数不够全面（可以使用多容器通信 也可以使用--link）

解决：

1. docker pull mysql:8.0.30

2. docker network create xxl-job

3. docker run -d --network xxl-job -p 3307:3306 --name mysql8  -v E:\docker_logs\mysql:/var/log/mysql  -v E:\docker\mysql\data:/var/lib/mysql  -v E:\docker\mysql\conf:/etc/mysql/conf.d  -e MYSQL_ROOT_PASSWORD=root  mysql:8.0.30

   可能的问题：mysqld: Can't read dir of '/etc/mysql/conf.d/' (OS errno 2 - No such file or directory) 

   解决：

   1. 先去掉 -v E:\docker\mysql\conf:/etc/mysql 启动
   2. 拷贝mysql容器中的my.cnf到主机中   docker cp  mysql8:/etc/mysql/conf.d E:\docker\mysql\
   3. docker rm mysql8
   4. 重新run

4. docker run -e PARAMS="--spring.datasource.url=jdbc:mysql://mysql8/xxl_job?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&serverTimezone=Asia/Shanghai --spring.datasource.username=root --spring.datasource.password=root" -p 9090:8080 -v E:\docker_logs:/data/applogs --name xxl-job-admin  -d  --network xxl-job xuxueli/xxl-job-admin:2.3.0 

   127.0.0.1:3306->mysql8

   添加 --spring.datasource.username=root --spring.datasource.password=password

5. http://localhost:9090/xxl-job-admin/toLogin

补充说明：

| `--link` |                                                              | Add link to another container |
| -------- | ------------------------------------------------------------ | ----------------------------- |
|          | `--link <name or id>:alias` 其中，name和id是源容器的name和id，alias是源容器在link下的别名。 |                               |

### 配置部署执行器（需要在自己的项目上部署）

1. 确认pom文件中引入了 “xxl-job-core” 的maven依赖；

   ```
   <dependency>
       <groupId>com.xuxueli</groupId>
       <artifactId>xxl-job-core</artifactId>
       <version>2.4.0-SNAPSHOT</version>
   </dependency>
   ```

   

2. 拷贝配置文件中需要的配置E:\xxl-job\xxl-job-executor-samples\xxl-job-executor-sample-springboot\src\main\resources\application.properties

   ```
   ### 调度中心部署根地址 [选填]：如调度中心集群部署存在多个地址则用逗号分隔。执行器将会使用该地址进行"执行器心跳注册"和"任务结果回调"；为空则关闭自动注册；
   xxl.job.admin.addresses=http://127.0.0.1:8080/xxl-job-admin
   ### 执行器通讯TOKEN [选填]：非空时启用；
   xxl.job.accessToken=
   ### 执行器AppName [选填]：执行器心跳注册分组依据；为空则关闭自动注册
   xxl.job.executor.appname=xxl-job-executor-sample
   ### 执行器注册 [选填]：优先使用该配置作为注册地址，为空时使用内嵌服务 ”IP:PORT“ 作为注册地址。从而更灵活的支持容器类型执行器动态IP和动态映射端口问题。
   xxl.job.executor.address=
   ### 执行器IP [选填]：默认为空表示自动获取IP，多网卡时可手动设置指定IP，该IP不会绑定Host仅作为通讯实用；地址信息用于 "执行器注册" 和 "调度中心请求并触发任务"；
   xxl.job.executor.ip=
   ### 执行器端口号 [选填]：小于等于0则自动获取；默认端口为9999，单机部署多个执行器时，注意要配置不同执行器端口；
   xxl.job.executor.port=9999
   ### 执行器运行日志文件存储磁盘路径 [选填] ：需要对该路径拥有读写权限；为空则使用默认路径；
   xxl.job.executor.logpath=/data/applogs/xxl-job/jobhandler
   ### 执行器日志文件保存天数 [选填] ： 过期日志自动清理, 限制值大于等于3时生效; 否则, 如-1, 关闭自动清理功能；
   xxl.job.executor.logretentiondays=30
   ```

   

   E:\xxl-job\xxl-job-executor-samples\xxl-job-executor-sample-springboot\src\main\java\com\xxl\job\executor\core\config

   ```
   @Bean
   public XxlJobSpringExecutor xxlJobExecutor() {
       logger.info(">>>>>>>>>>> xxl-job config init.");
       XxlJobSpringExecutor xxlJobSpringExecutor = new XxlJobSpringExecutor();
       xxlJobSpringExecutor.setAdminAddresses(adminAddresses);
       xxlJobSpringExecutor.setAppname(appname);
       xxlJobSpringExecutor.setIp(ip);
       xxlJobSpringExecutor.setPort(port);
       xxlJobSpringExecutor.setAccessToken(accessToken);
       xxlJobSpringExecutor.setLogPath(logPath);
       xxlJobSpringExecutor.setLogRetentionDays(logRetentionDays);
       return xxlJobSpringExecutor;
   }
   ```

   

3. 部署到docker上

4. 执行器集群(可选)



### Hello World

**配置执行器**

1.

```
<dependency>
    <groupId>com.xuxueli</groupId>
    <artifactId>xxl-job-core</artifactId>
    <version>2.4.0-SNAPSHOT</version>
</dependency>
```

2.

```
xxl:
  job:
    admin:
      addresses: http://localhost:9090/xxl-job-admin/
    executor:
      #注意：要与执行器管理中已有的appname对应,否则需要在后台创建执行器
      appname: my-xxl-job-executor-sample
      # 执行器所在主机的日志路径
      logpath: E:\xxl-job-logs
```

3.

```
@Slf4j
@Configuration
public class ExecutorConfig {
    @Value("${xxl.job.admin.addresses}")
    private String adminAddresses;

    @Value("${xxl.job.executor.appname}")
    private String appname;

    @Value("${xxl.job.executor.logpath}")
    private String logPath;

    @Bean
    public XxlJobSpringExecutor xxlJobExecutor() {
        log.info(">>>>>>>>>>> xxl-job config init.");
        XxlJobSpringExecutor xxlJobSpringExecutor = new XxlJobSpringExecutor();
        xxlJobSpringExecutor.setAdminAddresses(adminAddresses);
        xxlJobSpringExecutor.setAppname(appname);

        xxlJobSpringExecutor.setLogPath(logPath);
        return xxlJobSpringExecutor;
    }
```



**新建任务**

![1660225662028](assets/1660225662028.png)

```
任务配置：
    - 运行模式：
        BEAN模式：任务以JobHandler方式维护在执行器端；需要结合 "JobHandler" 属性匹配执行器中任务；
        GLUE模式(Java)：任务以源码方式维护在调度中心；该模式的任务实际上是一段继承自IJobHandler的Java类代码并 "groovy" 源码方式维护，它在执行器项目中运行，可使用@Resource/@Autowire注入执行器里中的其他服务；
        GLUE模式(Shell)：任务以源码方式维护在调度中心；该模式的任务实际上是一段 "shell" 脚本；
        GLUE模式(Python)：任务以源码方式维护在调度中心；该模式的任务实际上是一段 "python" 脚本；
        GLUE模式(PHP)：任务以源码方式维护在调度中心；该模式的任务实际上是一段 "php" 脚本；
        GLUE模式(NodeJS)：任务以源码方式维护在调度中心；该模式的任务实际上是一段 "nodejs" 脚本；
        GLUE模式(PowerShell)：任务以源码方式维护在调度中心；该模式的任务实际上是一段 "PowerShell" 脚本；
    - JobHandler：运行模式为 "BEAN模式" 时生效，对应执行器中新开发的JobHandler类“@JobHandler”注解自定义的value值；
    - 执行参数：任务执行所需的参数；
```

​	**JobHandler**

	1. 无框架

```
1、开发一个继承自"com.xxl.job.core.handler.IJobHandler"的JobHandler类，实现其中任务方法。
2、手动通过如下方式注入到执行器容器。
​```
XxlJobExecutor.registJobHandler("demoJobHandler", new DemoJobHandler());
​```
```

2. 方法形式

   ```
   1、任务开发：在Spring Bean实例中，开发Job方法；
   2、注解配置：为Job方法添加注解 "@XxlJob(value="自定义jobhandler名称", init = "JobHandler初始化方法", destroy = "JobHandler销毁方法")"，注解value值对应的是调度中心新建任务的JobHandler属性的值。
   ```

   ```
   @Component
   public class MyXxlJob {
       //与调度中心任务配制的名称相同
       @XxlJob("MyXxlJobHandler")
       public void demoJobHandler() throws Exception {
           XxlJobHelper.log("XXL-JOB, Hello World.");
       }
   }
   ```

   提示 xxl-job register jobhandler success 表示注册jobHandler成功

   

**执行任务**

- 执行一次
- 启动



**日志**

- 控制台
- 调度中心线上日志 任务-查看日志-调度备注
- 执行主机日志文件 logpath: E:\xxl-job-logs

### 路由策略

### 堵塞处理策略



























# 分布式限流

 服务器1s能处理10个请求

实际: 1s有100个请求

1. 新增服务器
2. 拒绝策略
3. 排队10s



## 压测(ab)

Apache Bench 是 Apache 服务器自带的一个web[压力测试](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3D%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95%26spm%3D1001.2101.3001.7020)工具，简称 ab 。

ab的原理：ab命令会创建 **多个并发** 访问线程，模拟 **多个访问者** 同时对某一 **URL地址** 进行访问。

它的测试目标是基于URL的，因此，它既可以用来测试apache的负载压力，也可以测试nginx、lighthttp、tomcat、[IIS](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3DIIS%26spm%3D1001.2101.3001.7020)等其它Web服务器的压力。

ab是一个命令行工具， ab命令对发出负载的计算机要求很低，它既不会占用很高CPU，也不会占用很多内存(安装到windows页没关系)



https://www.apachelounge.com/download/



**参数介绍**

```text
格式：ab [options] [http://]hostname[:port]/path

下面是参数

-n requests Number of requests to perform             //本次测试发起的总请求数
-c concurrency Number of multiple requests to make　　 //一次产生的请求数（或并发数） 模拟多人同时访问
-t timelimit Seconds to max. wait for responses　　　　//测试所进行的最大秒数，默认没有时间限制。
-r Don't exit on socket receive errors.              // 抛出异常继续执行测试任务 
-p postfile File containing data to POST　　    //包含了需要POST的数据的文件，文件格式如“p1=1&p2=2”.使用方法是 -p 111.txt

-T content-type Content-type header for POSTing
// POST 数据所使用的 Content-type 头信息，如 -T “application/x-www-form-urlencoded” 。 （配合-p）

-v verbosity How much troubleshooting info to print
//设置显示信息的详细程度 – 4或更大值会显示头信息， 3或更大值可以显示响应代码(404, 200等), 2或更大值可以显示警告和其他信息。 -V 显示版本号并退出。

-C attribute Add cookie, eg. -C “c1=1234,c2=2,c3=3” (repeatable)

//-C cookie-name=value 对请求附加一个Cookie:行。 其典型形式是name=value的一个参数对。此参数可以重复，用逗号分割。
提示：可以借助session实现原理传递 JSESSIONID参数， 实现保持会话的功能，如-C ” c1=1234,c2=2,c3=3, JSESSIONID=FF056CD16DA9D71CB131C1D56F0319F8″ 。

-w Print out results in HTML tables　　//以HTML表的格式输出结果。默认时，它是白色背景的两列宽度的一张表。
-i Use HEAD instead of GET
```

同时处理c个请求并运行n次url文件





**输出指标分析(ab->hrrp abs->https)**

![image-20220809092704657](assets/image-20220809092704657.png)

Document Path:测试页面
Document Length: 页面大小
Concurrency Level: 测试的[并发](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3D%E5%B9%B6%E5%8F%91%26spm%3D1001.2101.3001.7020)数
Time taken for tests:整个测试持续的时间
Complete requests:完成的请求数量
Failed requests: 失败的请求数量

Total transferred: 整个过程中的网络传输量
HTML transferred: 整个过程中的HTML内容传输量
Requests per second: 最重要的指标之一，相当于LR中的每秒事务数，后面括号中的mean表示这是一个平均值
Time per request: 最重要的指标之二，相当于LR中的平均事务响应时间，后面括号中的mean表示这是一个平均值,代表每个链接上单个请求的平均响应时间
Time per request: 所有链接合计后单个请求的平均响应时间

​			Tpr1=Tpr2*c

Transfer rate: 平均每秒网络上的流量，可以帮助排除是否存在网络流量过大导致响应时间延长的问题



测试

1. n 100

   c100 Requests per second:    19.08 [#/sec] (mean)

   c90 Requests per second:    10.32 [#/sec] (mean)

   c80 Requests per second:    11.91 [#/sec] (mean)
   c65 Requests per second:    14.87 [#/sec] (mean)
   c57 Requests per second:    16.98 [#/sec] (mean)
   c50 Requests per second:    17.88 [#/sec] (mean)
   c20 Requests per second:    15.91 [#/sec] (mean)
   c5 Requests per second:    14.47 [#/sec] (mean)
   c1 Requests per second:    4.62 [#/sec] (mean)

   并发数在1/2n附近或n为宜

2. c 50

   n800 Requests per second:    17.62 [#/sec] (mean)
   n400 Requests per second:    17.15 [#/sec] (mean)
   n200 Requests per second:    16.98 [#/sec] (mean)
   n100 Requests per second:    17.88 [#/sec] (mean)
   n50  Requests per second:    16.98 [#/sec] (mean)

   并发数一定,发现qps基本相同



**测试自己的rest Api** 

**ab无论错误有结果就会返回,需要提前先测下接口**

ab -c 50 -n 100 http://localhost:8080/leaf/segment   1601.51[#/sec] (mean) failed 0  由于step增大 qps越来越高

ab -c 100 -n 100 http://localhost:8080/leaf/segment 1501.40 [#/sec] (mean)  failed 取决于step step越小failed越大



模拟处理延迟

```
@GetMapping("/testAb")
public long testAb()  {
    Thread.sleep(500);
    return 1L;
}
```

ab -c 50 -n 100 http://localhost:8080/leaf/testAb   63.75 [#/sec] (mean)

ab -c 10 -n 100 http://localhost:8080/leaf/testAb    84.62 [#/sec] (mean)

考虑通过异步提高qps

ab -c 50 -n 100 http://localhost:8080/leaf/testAb  1990[#/sec] (mean)

ab -c 10 -n 100 http://localhost:8080/leaf/testAb   2135.57 [#/sec] (mean)







## 算法

### 自研算法

#### 算法一 计数器

```
/**
 * 单位时间(T)内 允许最大流量 ReqMax,维护计数器Cnt
 * 请求进来,判断是否到下一个单位时间
 *      1. 否  Cnt++ 若 Cnt>ReqMax 拒绝请求
 *      2. 是  Cnt=0
 */
public static void main(String[] args) {
    final int T=5;
    final int REQ_MAX=3;
    int cnt=0;
    long startTime=System.currentTimeMillis();
    long requestTime=startTime;

    while (true){
        requestTime= requestTime+ new Random().nextInt(2*T*1000);
        if(requestTime>startTime+5000){
            cnt=1;
            startTime=System.currentTimeMillis();
            requestTime=startTime;
            System.out.println(requestTime+" 接受请求");
        }else{
            cnt++;
            if(cnt>REQ_MAX){
                System.out.println("当前cnt为 "+cnt+" ,拒绝请求");
                continue;
            }
            System.out.println(requestTime+" 接受请求");
        }
    }
```

存在的问题

在两个单位时间的临界值上的处理是有缺陷的。

第一个单位时间的最后一秒里达到的请求数为ReqMax(之前没有请求),接下来第二个单位时间T的第一秒里达到请求数也是ReqMax,由于超时重置发生在两个单位时间之间，所以这2*ReqMax个请求都将通过控制，也就是说在2s里处理2*ReqMax个请求，与我们设置的T里ReqMax个请求的设想是相违背

学术一点的说法是**该算法处理请求不够平滑**，不能很好的满足限流需求



#### 滑动窗口

为了解决处理请求不够平滑

将一个窗口分为若干个等份的小窗口，每个小窗口对应不同的时间点，拥有独立的计数器，当请求的时间点大于当前窗口的最大时间点时，则将窗口向前平移一个小窗口,整个窗口的所有请求数相加不能大于阀值.

1. 比如说滑动窗口是1min,划分成10个格子,即每个6s

2. 每过6秒，滑动窗口向右移动1个格子

3. 每个格子都有独立的计数器
4. 如果时间窗口内所有的计数器之和超过了限流阀值，则触发限流操作
5. 滑动窗口设置得越精细，限流的效果越好，但滑动窗口的时间间隔（小格子）小了，存储的空间也会增加。



```
public class SlideWindow {
    //时间间隔
    private static final int delimiter=6;
    //window 6s*10
    private static final int[] window={0,0,0,0,0,0,0,0,0,0};
    //window当前的指针
    private static int curId=0;
    //window阈值
    private static final long threshold=100;
    //counter
    private static long cnt=0;
    //上次time
    private static long lastTime=System.currentTimeMillis();


    public static boolean slideWindowLimit(){
        long now=System.currentTimeMillis();
        long speriod=(now-lastTime)/1000;
        if(speriod>=delimiter){ //移动到下一个格子
            curId++;
            curId=curId%window.length; //循环数组
            cnt-=window[curId];  //覆盖最早的计数
            window[curId]=1;
            lastTime=now;
        }else{
            ++window[curId];
        }
        cnt++;
        return cnt<=threshold;
    }

    @SneakyThrows
    public static void main(String[] args) {
        for (int i = 0; i < 11*6; i++) {  //
            Thread.sleep(1000);
            for (int j = 0; j < new Random().nextInt(10)+1; j++) {
                if(slideWindowLimit()){
                    System.out.println("没限流 "+cnt);
                }else{
                    System.out.println("开始限流 "+cnt);
                }
            }
        }

    }
}
```

问题:

如果窗口前期时间请求过多快速达到阈值,会导致后期请求均被拒接

1. 后期拒绝的请求不计入cnt
2. 每个时间结点限流,比如一个窗口1w个请求,那么一个格子最多1w/10







### 漏桶算法

漏桶算法思路很简单，请求先进入到漏桶里，漏桶以固定的速度出水，也就是处理请求，当水加的过快，则会直接溢出，也就是拒绝请求，可以看出**漏桶算法能强行限制数据的传输速率**。

![img](assets/v2-a7dbe4f48d28d90b378a64a5ff19b33a_1440w.jpg)

```
//桶的容量
final long capacity=5000;
//水漏出的速率
final int rate=1;
//当前桶里的数量
long water=2000;
long prev=System.currentTimeMillis();
long now;

while (true){
    now=System.currentTimeMillis();
    //先执行漏水,(now-prev)*rate 可以看作这段时间的流出
    water=Math.max(0,water-(now-prev)*rate);
    prev=now;
    if(water<capacity){  //水还未满 加水
        water++;
        System.out.println("水还未满 加水");
    }else{
        System.out.println("水桶已满,拒绝");
    }

}
```

优点:

1. 很好的解决了时间边界处理不够平滑的问题

2. **能够强行限制数据的传输速率**

缺点:

无法处理某种程度的突发传输

###　令牌桶算法

令牌桶算法的原理是系统会以一个恒定的速度往桶里放入令牌，而如果请求需要被处理，则需要先从桶里获取一个令牌，当桶里没有令牌可取时，则拒绝服务。

![image-20220809144854717](assets/image-20220809144854717.png)

- 假如用户配置的平均发送速率为r，则每隔1/r秒一个令牌被加入到桶(每秒会有r个令牌放入桶中)；
- 假设桶中最多可以存放b个令牌。如果令牌到达时令牌桶已经满了，那么这个令牌会被丢弃
- 算法允许最长b个请求的突发，但从长期运行结果看，发送速率的速率被限制成常量r



```
//令牌桶的容量
final long capacity=5000;
//桶里令牌数
long tokens=1000;
//令牌生成速率 r/ms个
int r=300;
//每次需要获取的令牌
int tryAcquire;
//之前突发的债务
long remain=0;
long prev = System.currentTimeMillis();
long now;

while (true){
    now=System.currentTimeMillis();
    tryAcquire=1+new Random().nextInt((int)(capacity*1.9));
    //先获取令牌
    tokens=Math.min(capacity,tokens+remain+(now-prev)*r);
    prev=now;
    if(tokens>=0){
        if(tryAcquire<=tokens){
            tokens-=tryAcquire;
            System.out.println(tryAcquire+" 正常获取 tokens: "+tokens);
        }
        else if(tryAcquire<=tokens+capacity) {//允许突发
            remain=tokens-tryAcquire;
            tokens=0;
            System.out.println(tryAcquire+" 允许突发,突发了 "+remain);
        }else{
            System.out.println("请求量太大,不允许突发 "+tryAcquire+" 作废");
        }
    }else{
        System.out.println("等待速率整型");
        Thread.sleep(-tokens/r);
        tokens=0;
        remain=0;
    }

}
```







## 限流工具包 RateLimiter

**单机版限流**

google开源工具包guava提供了限流工具类RateLimiter，该类基于“**令牌桶算法**”，非常方便使用。RateLimiter经常用于限制对一些物理资源或者逻辑资源的访问速率。它支持两种获取permits接口，一种是如果拿不到立刻返回false，一种会阻塞等待一段时间看能不能拿到。

```
<dependency>
    <groupId>com.google.guava</groupId>
    <artifactId>guava</artifactId>
    <version>30.0-jre</version>
</dependency>
```



1. 创建令牌桶

   **初始化令牌1**

   ```
   RateLimiter limiter = RateLimiter.create(3);//permitsPerSecond
   ```

2. 获取令牌

   - ```
     //平滑输出
     log.info("{}",limiter.acquire()); 0.0
     log.info("{}",limiter.acquire()); 0.33
     ```

   - ```
     //允许突发
     log.info("{}",limiter.acquire(4)); 0.0
     log.info("{}",limiter.acquire()); 1.33=(4-1)*1/3+1/3		
     ```

```
log.info("{}",limiter.acquire(4)); //0.0
log.info("{}",limiter.acquire(6)); //1.33
log.info("{}",limiter.acquire()); //2.33
```

总结: 后者时间=前者复发债务+1/r



- ```text
  //以非阻塞的形式达到降级
  //可以实现 1+permitsPerSecond个秒杀效果(高并发下)
  if(limiter.tryAcquire()) { //未请求到limiter则立即返回false
      doSomething();
  }else{
      doSomethingElse();
  }
  ```





## 分布式-sentinel

https://github.com/alibaba/sentinel 

​	Documentation->中文文档

​	快速开始->主流框架适配

https://github.com/alibaba/spring-cloud-alibaba/wiki/Sentinel

随着分布式系统变得越来越流行，服务之间的可靠性变得比以往任何时候都更加重要。[Sentinel](https://sentinelguard.io/)是一个强大的流控组件，以“流”为切入点，涵盖流控、并发限制、断路、自适应系统保护等多个领域，保障微服务的可靠性

![图片](assets/69955207-1e5d3c00-1538-11ea-9ab2-297efff32809.png)



## Hello World

```
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-devtools</artifactId>
</dependency>
```

问题 spring boot 版本与spring cloud不兼容

1. change spring boot  version
2. spring​.cloud:.compatibility-verifier.enabled: false





### 控制台

https://github.com/alibaba/spring-cloud-alibaba/wiki/Sentinel

















# 分库分表

## ShardingSphere-jdbc

https://shardingsphere.apache.org/document/5.0.0/cn/overview/

SQL方言（或者数据库方言）指的是**用于访问数据库的结构化查询语言的变体**，根据具体的数据库系统不同，也可能会支持不同的方言。 简单而言，**某种DBMS不只会支持SQL标准，而且还会有一些自己独有的语法，这就称之为方言**。 例如LIMIT语句只能在MySQL中使用。

数据库协议:不同数据库的服务器和客户端的通信协议不同,比如oracle TNS,SQL Server TDS

SQL 方言和数据库协议的兼容，是数据库兼容度提升的关键点。



## SB集成

1. ```
   <dependency>
       <groupId>org.apache.shardingsphere</groupId>
       <artifactId>shardingsphere-jdbc-core-spring-boot-starter</artifactId>
       <version>5.0.0</version>
   </dependency>
           <dependency>
               <groupId>org.springframework.boot</groupId>
               <artifactId>spring-boot-starter-jdbc</artifactId>
           </dependency>
           <dependency>
               <groupId>mysql</groupId>
               <artifactId>mysql-connector-java</artifactId>
               <version>8.0.18</version>
           </dependency>
   ```

2. ```
   create database ds0;
   use ds0;
   create table t_order_0(
   	order_id bigint unsigned not null,
   	user_id int(11) default null,
   	primary key(order_id)
   )engine=innodb,default charset=utf8;
   create table t_order_1(
   	order_id bigint unsigned not null,
   	user_id int(11) default null,
   	primary key(order_id)
   )engine=innodb,default charset=utf8;
   
   create database ds1;
   use ds1;
   create table t_order_0(
   	order_id bigint unsigned not null,
   	user_id int(11) default null,
   	primary key(order_id)
   )engine=innodb,default charset=utf8;
   create table t_order_1(
   	order_id bigint unsigned not null,
   	user_id int(11) default null,
   	primary key(order_id)
   )engine=innodb,default charset=utf8;
   ```

3. ```
   # 配置真实数据源
   spring.shardingsphere.datasource.names=ds0,ds1
   
   # 配置第 1 个数据源
   spring.shardingsphere.datasource.ds0.type=com.zaxxer.hikari.HikariDataSource
   spring.shardingsphere.datasource.ds0.driver-class-name=com.mysql.cj.jdbc.Driver
   spring.shardingsphere.datasource.ds0.jdbc-url=jdbc:mysql://localhost:3306/ds0?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&serverTimezone=Asia/Shanghai
   spring.shardingsphere.datasource.ds0.username=root
   spring.shardingsphere.datasource.ds0.password=123456
   
   # 配置第 2 个数据源
   spring.shardingsphere.datasource.ds1.type=com.zaxxer.hikari.HikariDataSource
   spring.shardingsphere.datasource.ds1.driver-class-name=com.mysql.cj.jdbc.Driver
   spring.shardingsphere.datasource.ds1.jdbc-url=jdbc:mysql://localhost:3306/ds1?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&serverTimezone=Asia/Shanghai
   spring.shardingsphere.datasource.ds1.username=root
   spring.shardingsphere.datasource.ds1.password=123456
   
   # 配置 t_order 表规则
   spring.shardingsphere.rules.sharding.tables.t_order.actual-data-nodes=ds$->{0..1}.t_order$->{0..1}
   
   # 配置分库策略 根据user_id决定进入哪个库
   spring.shardingsphere.rules.sharding.tables.t_order.database-strategy.standard.sharding-column=user_id
   spring.shardingsphere.rules.sharding.tables.t_order.database-strategy.standard.sharding-algorithm-name=database_inline
   
   # 配置分表策略
   spring.shardingsphere.rules.sharding.tables.t_order.table-strategy.standard.sharding-column=order_id
   spring.shardingsphere.rules.sharding.tables.t_order.table-strategy.standard.sharding-algorithm-name=table_inline
   
   # 配置 分片算法
   # 更多算法在 配置手册-内置算法-分片算法
   spring.shardingsphere.rules.sharding.sharding-algorithms.database_inline.type=INLINE
   spring.shardingsphere.rules.sharding.sharding-algorithms.database_inline.props.algorithm-expression=ds_${user_id % 2}
   spring.shardingsphere.rules.sharding.sharding-algorithms.table_inline.type=INLINE
   spring.shardingsphere.rules.sharding.sharding-algorithms.table_inline.props.algorithm-expression=t_order_${order_id % 2}
   # 分布式序列策略配置
   spring.shardingsphere.rules.sharding.tables.t_order.key-generate-strategy.column=order_id # 分布式序列列名称
   spring.shardingsphere.rules.sharding.tables.t_order.key-generate-strategy.key-generator-name=my-snowflake # 分布式序列算法名称
   
   # 分布式序列算法配置
   spring.shardingsphere.rules.sharding.key-generators.my-snowflake.type=SNOWFLAKE # 分布式序列算法类型
   spring.shardingsphere.rules.sharding.key-generators.my-snowflake.props.worke-id=123 # 分布式序列算法属性配置
   ```

问题1:Error creating bean with name 'database_inline': Initialization of bean failed; nested exception is java.lang.NullPointerException: Inline sharding algorithm expression cannot be null.

原因:spring2.x _是无效字符 应该用kebab-case

database_inline->database-inline table_inline->table-inline

源码在`ConfigurationPropertiesBindingPostProcessor`



问题2: unresovled user_id % 2 in {user_id % 2}

原因:数据库错误 ds$->{user_id % 2} 而不是ds_${user_id % 2}



问题3: 增加分布式id时 org.apache.shardingsphere.spi.exception.ServiceProviderNotFoundException: No implementation class load from SPI `org.apache.shardingsphere.sharding.spi.KeyGenerateAlgorithm` with type `SNOWFLAKE  # 分布式序列算法类型

可以替换成(不推荐)

```
spring.shardingsphere.sharding.tables.t_order.key-generator.column=order_id # 分布式序列列名称
spring.shardingsphere.sharding.tables.t_order.key-generator.type=SNOWFLAKE # 分布式序列列名称
spring.shardingsphere.sharding.tables.t_order.key-generator.props.worker-id=123 # 分布式序列列名称
```

转成yaml

```
spring:
  shardingsphere:
    datasource:
      ds0:
        driver-class-name: com.mysql.cj.jdbc.Driver
        jdbc-url: jdbc:mysql://localhost:3306/ds0?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&serverTimezone=Asia/Shanghai
        password: 123456
        type: com.zaxxer.hikari.HikariDataSource
        username: root
      ds1:
        driver-class-name: com.mysql.cj.jdbc.Driver
        jdbc-url: jdbc:mysql://localhost:3306/ds1?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&serverTimezone=Asia/Shanghai
        password: 123456
        type: com.zaxxer.hikari.HikariDataSource
        username: root
      names: ds0,ds1
    rules:
      sharding:
        key-generators:
          my:
            props:
              worker-id: 123 # 分布式序列算法属性配置
            type: SNOWFLAKE  # 分布式序列算法类型
        sharding-algorithms:
          database-inline:
            props:
              algorithm-expression: ds$->{user_id % 2}
            type: INLINE
          table-inline:
            props:
              algorithm-expression: t_order_$->{order_id % 2}
            type: INLINE
        tables:
          t_order:
            actual-data-nodes: ds$->{0..1}.t_order_$->{0..1}
            database-strategy:
              standard:
                sharding-algorithm-name: database-inline
                sharding-column: user_id
            key-generate-strategy:
              column: order_id  #分布式序列列名称
              key-generator-name: my # 分布式序列算法名称
            table-strategy:
              standard:
                sharding-algorithm-name: table-inline
                sharding-column: order_id
```

4. 

   ```
   @Entity
   @Data
   @AllArgsConstructor
   @NoArgsConstructor
   public class OrderEntity {
       @Id
       private Long orderId;
       private Integer userId;
   }
   ```

   问题: Table 'ds0.order_entity' doesn't exist

   原因: 没有指定 OrderEntity->对应的表就是order_entity

   添加 @Table(name = "t_order")

5. 日志集成

- logging.level=debug

  logging.level.package=debug

* sharding自带

  ```properties
  spring.shardingsphere.props.xxx.xxx= # 具体的属性配置
  spring.shardingsphere.props.sql-show: true
  ```



结果:

Logic SQL: insert into t_order (user_id, order_id) values (?, ?)

Actual SQL: ds0 ::: insert into t_order_1 (user_id, order_id) values (?, ?) ::: [534, 1660265434315]



补充: 

之前已经为t_order表设计了key-generate-strategy

自定义主键值会覆盖生成的雪花算法,可以无序定义主键值

问题:ids for this class must be manually assigned before calling save()

添加 @GeneratedValue(strategy=GenerationType.IDENTITY)



查询

根据userId能确定具体的库,orderId能确定具体的表



## 读写分离
**实现读写分离的两种方式**

**限制从库只能读，数据从主库更换**

注意点： 

1. docker-compose.yml要从windows格式转到unix

2. mysql -uroot -p  -P 3307  密码是宿主机的root@mysql的密码而不是

   docker container of mysql的密码



**测试主从同步是否生效**

在master中执行脚本

```
create database test_db;
use test_db;
create table test_tab(
	id int not null,
	primary key(id)
)engine=innodb,default charset=utf8mb4;
insert into test_tab(id) values(111);
```

连接slave 看是否存在同步的库表数据

**通过shardingsphere组件**

```
spring.shardingsphere.rules.readwrite-splitting.data-sources.<readwrite-splitting-data-source-name>.write-data-source-name= # 写数据源名称
spring.shardingsphere.rules.readwrite-splitting.data-sources.<readwrite-splitting-data-source-name>.read-data-source-names= # 读数据源名称，多个从数据源用逗号分隔
spring.shardingsphere.rules.readwrite-splitting.data-sources.<readwrite-splitting-data-source-name>.load-balancer-name= # 负载均衡算法名称

# 负载均衡算法配置
spring.shardingsphere.rules.readwrite-splitting.load-balancers.<load-balance-algorithm-name>.type= # 负载均衡算法类型
spring.shardingsphere.rules.readwrite-splitting.load-balancers.<load-balance-algorithm-name>.props.xxx= # 负载均衡算法属性配置
```



```Failed to determine a suitable driver class
spring:
  shardingsphere:
    datasource:
      common:
        driver-class-name: com.mysql.cj.jdbc.Driver
        password: d3eb23f714529f1e73f934876d1b39
        type: com.zaxxer.hikari.HikariDataSource
        username: root
      names: primary-ds,replice-ds-0
      primary-ds:
        jdbc-url: jdbc:mysql://192.168.162.100:3307/test_db?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&serverTimezone=Asia/Shanghai
      replice-ds-0:
        jdbc-url: jdbc:mysql://192.168.162.100:3308/test_db?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&serverTimezone=Asia/Shanghai

#    datasource:
#      names: primary-ds,replice-ds-0
#      primary-ds:
#        type: com.zaxxer.hikari.HikariDataSource
#        password: d3eb23f714529f1e73f934876d1b39
#        driver-class-name: com.mysql.cj.jdbc.Driver
#        jdbc-url: jdbc:mysql://192.168.162.100:3307/test_db?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&serverTimezone=UTC
#        username: root
#      replice-ds-0:
#        type: com.zaxxer.hikari.HikariDataSource
#        password: d3eb23f714529f1e73f934876d1b39
#        driver-class-name: com.mysql.cj.jdbc.Driver
#        jdbc-url: jdbc:mysql://192.168.162.100:3308/test_db?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&serverTimezone=UTC
#        username: root

    rules:
      replica-query:
        data-sources:
          # <readwrite-splitting-data-source-name> 名称pr_ds可以自定义
          pr_ds:
            load-balancer-name: round-robin # 负载均衡算法名称
     		replica-data-source-names: replice-ds-0 # 读数据源名称，多个从数据源用逗号分隔
            primary-data-source-name: primary-ds # 写数据源名称'
        load-balancers:
          round-robin:
            type: ROUND_ROBIN # 负载均衡算法类型
            props:
              default: 0
    props:
      sql-show: true

```

问题： Failed to determine a suitable driver class

原因： 

1. url有误  192.168.162.100:3308/test_db

2. dataAutoConfiguration  

   ```
   exclude = {DataSourceAutoConfiguration.class}
   ```

3. **springboot应该导入的jar包错了**

   ```
   <dependency>
       <groupId>org.apache.shardingsphere</groupId>
       <artifactId>shardingsphere-jdbc-core-spring-boot-starter</artifactId>
       <version>5.0.0-alpha</version>
   </dependency>
   ```

问题2：Access to DialectResolutionInfo cannot be null when ‘hibernate.dialect‘ not set

```
# the spring.jpa.generate-ddl is a single switch that is applicable to hibernate or any other JPA vendor

spring.jpa.generate-ddl=true
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQLDialect
```

3. 问题： **java.sql.SQLNonTransientConnectionException: Could not create connection to database server. Attempted reconnect 3 times. Giving up.**

   passwd d3eb23f714529f1e73f934876d1b39

   之前linux有了一个mysql，docker-poxy又运行两个容器，里面有mysql，这两个不是同一个库

**测试**

1. 插入

   replice-ds-0 ::: select testentity0_.id as id1_0_0_ from test_tab testentity0_ where testentity0_.id=? ::: [10087]

   Actual SQL: primary-ds ::: select testentity0_.id as id1_0_0_ from test_tab testentity0_ where testentity0_.id=? ::: [10087]

2. 查询

   replice-ds-0 ::: select testentity0_.id as id1_0_ from test_tab testentity0_

3. 强制从主库查询

   ```
   HintManager.getInstance().setPrimaryRouteOnly();
   testRepository.findById(10087);
   ```

   Actual SQL: primary-ds ::: select testentity0_.id as id1_0_0_ from test_tab testentity0_ where testentity0_.id=? ::: [10087]
>>>>>>> 1dacb99c13e6fd11e81d01889fa8ce14e876378a



# 分布式事务

![image-20220812145153787](assets/image-20220812145153787.png)



**SEATA**

https://seata.io/zh-cn/

Seata 是一款开源的分布式事务解决方案，致力于提供高性能和简单易用的分布式事务服务。Seata 将为用户提供了 AT、TCC、SAGA 和 XA 事务模式



**AT**

**前提**

- 基于支持本地 ACID 事务的关系型数据库。
- Java 应用，通过 JDBC 访问数据库



**整体机制**

两阶段提交协议的演变：

- 一阶段：业务数据和回滚日志记录在同一个本地事务中提交，释放本地锁和连接资源。

- 二阶段：

  - 提交异步化，非常快速地完成。
  - 回滚通过一阶段的回滚日志进行反向补偿。

  

**写隔离**

- 一阶段本地事务提交前，需要确保先拿到 **全局锁** 。
- 拿不到 **全局锁** ，不能提交本地事务。
- 拿 **全局锁** 的尝试被限制在一定范围内，超出范围将放弃，并回滚本地事务，释放本地锁。

以一个示例来说明：

两个全局事务 tx1 和 tx2，分别对 a 表的 m 字段进行更新操作，m 的初始值 1000。

tx1 先开始，开启本地事务，拿到本地锁，更新操作 m = 1000 - 100 = 900。本地事务提交前，先拿到该记录的 **全局锁** ，本地提交释放本地锁。 tx2 后开始，开启本地事务，拿到本地锁，更新操作 m = 900 - 100 = 800。本地事务提交前，尝试拿该记录的 **全局锁** ，tx1 全局提交前，该记录的全局锁被 tx1 持有，tx2 需要重试等待 **全局锁** 。

![Write-Isolation: Commit](assets/TB1zaknwVY7gK0jSZKzXXaikpXa-702-521.png)

tx1 二阶段全局提交，释放 **全局锁** 。tx2 拿到 **全局锁** 提交本地事务。

![Write-Isolation: Rollback](assets/TB1xW0UwubviK0jSZFNXXaApXXa-718-521.png)

如果 tx1 的二阶段全局回滚，则 tx1 需要重新获取该数据的本地锁，进行反向补偿的更新操作，实现分支的回滚。

此时，如果 tx2 仍在等待该数据的 **全局锁**，同时持有本地锁，则 tx1 的分支回滚会失败。分支的回滚会一直重试，直到 tx2 的 **全局锁** 等锁超时，放弃 **全局锁** 并回滚本地事务释放本地锁，tx1 的分支回滚最终成功。

因为整个过程 **全局锁** 在 tx1 结束前一直是被 tx1 持有的，所以不会发生 **脏写** 的问题。



**读隔离**

在数据库本地事务隔离级别 **读已提交（Read Committed）** 或以上的基础上，Seata（AT 模式）的默认全局隔离级别是 **读未提交（Read Uncommitted）** 。

如果应用在特定场景下，必需要求全局的 **读已提交** ，目前 Seata 的方式是通过 SELECT FOR UPDATE 语句的代理。

![Read Isolation: SELECT FOR UPDATE](assets/TB138wuwYj1gK0jSZFuXXcrHpXa-724-521.png)

SELECT FOR UPDATE 语句的执行会申请 **全局锁** ，如果 **全局锁** 被其他事务持有，则释放本地锁（回滚 SELECT FOR UPDATE 语句的本地执行）并重试。这个过程中，查询是被 block 住的，直到 **全局锁** 拿到，即读取的相关数据是 **已提交** 的，才返回。

出于总体性能上的考虑，Seata 目前的方案并没有对所有 SELECT 语句都进行代理，仅针对 FOR UPDATE 的 SELECT 语句。



## 

##　**未使用分布式事务**

**product端**

1.

```
create database seata_order;
use seata_order;
create table t_order(
	order_id bigint unsigned not null,
	product_id int default 0,
	primary key(order_id)
)engine=innodb,default charset=utf8mb4;

create database seata_product;
use seata_product;
create table t_product(
	product_id bigint unsigned not null,
	count int default 0,
	primary key(product_id)
)engine=innodb,default charset=utf8mb4;
insert  into t_product(product_id,count) values(10086,100);
```

2.

```
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-jpa</artifactId>
</dependency>
<dependency>
    <groupId>mysql</groupId>
    <artifactId>mysql-connector-java</artifactId>
    <version>8.0.18</version>
</dependency>
```

3.

```
spring.datasource.url=jdbc:mysql://127.0.0.1:3306/seata_product?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true&serverTimezone=Asia/Shanghai
spring.datasource.username=root
spring.datasource.password=123456
spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver
```

4.

```
@Entity
@Data
@Table(name="t_product")
public class Product {
    @Id
    @GeneratedValue(strategy=GenerationType.IDENTITY)
    private Long productId;

    private Integer count;
}
```

5.

```
public interface ProductRepository extends JpaRepository<Product,Long> {

}
```

6.

```
@Service
public class ProductService {
    @Autowired
    private ProductRepository prodcutDao;

    public void decrProduct(Long productId,Integer count){
        Optional<Product> productOptional = prodcutDao.findById(productId);
        if(productOptional.isPresent()){
            Product  product= productOptional.get();
            product.setCount(product.getCount()-count);
            prodcutDao.save(product);
        }
    }
}
```

7.

```
@RestController
@RequestMapping("/product")
public class ProductController {
    @Autowired
    private ProductService productService;

    @RequestMapping("decrProduct")
    public void decrProduct(@RequestParam("product_id") Long productId
                            ,@RequestParam("count") Integer count){
        productService.decrProduct(productId,count);
    }
}
```





**order端**

```
@Service
public class OrderService {
    @Autowired
    private RestTemplate restTemplate;

    public String placeOrder(){
        MultiValueMap<String, Object> uriVariable = new LinkedMultiValueMap<String, Object>();
        uriVariable.put("product_id", Collections.singletonList(10086));
        uriVariable.put("count", Collections.singletonList(10));
        restTemplate.put("http://localhost:8080/product/decrProduct"
                        ,uriVariable);
        int i=1/0;
        return "order success";
    }
}
```



存在的问题: Order端请求更新库存成功后,如果创建订单失败,并不会回滚库存,导致数据不一致



## @Transactional

**@Transactional对分布式事务不生效**

1. rollbackFor、rollbackForClassName用于设置那些异常需要回滚；noRollbackFor、noRollbackForClassName用于设置那些异常不需要回滚

2. 在使用@Transactional 的时候一定要记住，在private,protected方法上添加@Transactional 注解不会有任何效果。

3. 同个类函数间的调用

     同一个类AClass中，有两个函数aFunction、aInnerFunction。aFunction调用aInnerFunction。而且aFunction函数会被外部调用。

   - aFunction添加了@Transactional注解，aInnerFunction函数没有添加。aInnerFunction抛异常

     ```cpp
     
     ```
    public class AClass {
     
         @Transactional(rollbackFor = Exception.class)
         public void aFunction() {
             //todo: 数据库操作A(增，删，该)
             aInnerFunction(); // 调用内部没有添加@Transactional注解的函数
         }
     
         private void aInnerFunction() {
             //todo: 操作数据B(做了增，删，改 操作)
             throw new RuntimeException("函数执行有异常!");
         }
     
     }
     ```
       
     结果：两个函数操作的数据都会回滚。

   - 两个函数都添加了@Transactional注解。aInnerFunction抛异常。
   
     ```cpp
     public class AClass {
     
         @Transactional(rollbackFor = Exception.class)
         public void aFunction() {
             //todo: 数据库操作A(增，删，该)
             aInnerFunction(); // 调用内部没有添加@Transactional注解的函数
         }
     
         @Transactional(propagation = Propagation.REQUIRES_NEW, rollbackFor = Exception.class)
         private void aInnerFunction() {
             //todo: 操作数据B(做了增，删，改 操作)
             throw new RuntimeException("函数执行有异常!");
         }
     
     }
     ```
   
     Propagation.REQUIRES_NEW 表示当前方法必须运行在它自己的事务中。一个新的事务将被启动。如果存在当前事务，在该方法执行期间，当前事务会被挂起。 default REQUIRED
   
      结果：同第一种情况一样，两个函数对数据库操作都会回滚。因为同一个类中函数相互调用的时候，内部函数添加@Transactional注解无效。@Transactional注解只有外部调用才有效。

- aFunction不添加注解，aInnerFunction添加注解。aInnerFunction抛异常

     结果：两个函数对数据库的操作都不会回滚。因为内部函数@Transactional注解添加和没添加一样。

  没有经过 Spring 的代理类

- aFunction添加了@Transactional注解，aInnerFunction函数没有添加。aInnerFunction抛异常，不过在aFunction里面把异常抓出来了

  结果：两个函数里面的数据库操作都成功。事务回滚的动作发生在当**有@Transactional注解函数有对应异常抛出时才会回滚**。(当然了要看你添加的@Transactional注解有没有效,如上一中情况)。

4. 不同类函数间调用

   两个类AClass、BClass。AClass类有aFunction、BClass类有bFunction。AClass类aFunction调用BClass类bFunction。最终会在外部调用AClass类的aFunction。

   - aFunction添加注解，bFunction不添加注解。bFunction抛异常。

     结果：两个函数对数据库的操作都回滚了。

   - **aFunction、bFunction两个函数都添加注解，bFunction抛异常。**

     **结果：两个函数对数据库的操作都回滚了。**

   - aFunction、bFunction两个函数都添加注解，bFunction抛异常。aFunction抓出异常。

     - b是新的事务

       b回滚 a执行成功

     - 同一事务

       两个函数数据库操作都没成功。而且还抛异常了。org.springframework.transaction.UnexpectedRollbackException: Transaction rolled back because it has been marked as rollback-only。

       两个函数用的是同一个事务。bFunction函数抛了异常，调了事务的rollback函数。事务被标记了只能rollback了。程序继续执行，aFunction函数里面把异常给抓出来了，这个时候aFunction函数没有抛出异常，既然你没有异常那事务就需要提交，会调事务的commit函数。而之前已经标记了事务只能rollback-only(以为是同一个事务)。直接就抛异常了，不让调了

     
     
     
     
     ## Maven import scope
     
     1. This scope is only supported on a dependency of type pom in the <dependencyManagement> section
     
        只支持对pom文件<dependencyManagement>里的dependency
     
     2. It indicates the dependency is to be replaced with the effective list of dependencies in the specified POM's <dependencyManagement> section
     
        这个dependency 会被 引入pom<dependencyManagement> 下面dependencies有效清单替换 相当于复制黏贴,绕开了maven单继承的限制
     
     3. Since they are replaced, dependencies with a scope of import do not actually participate in limiting the transitivity of a dependency.
     
        这些dependency等价于直接在pom文件找那个定义,而不是从父项目中继承
   
   
   
   **Example**
   
   项目A和项目B的pom.xml文件中都定义了<dependencyManagement>，同时在B的<dependencyManagement>中定义<dependency>指向A，且[scope](https://so.csdn.net/so/search?q=scope&spm=1001.2101.3001.7020)为import
   
   这是project-B的pom.xml
     <dependencyManagement>
       <dependencies>
         <dependency>
           <groupId>project-A</groupId>
           <artifactId>project-A</artifactId>
           <version>1.0</version>
           <scope>import</scope>
         </dependency>
         ...
       </dependencies>
     </dependencyManagement>
   这就相当于将A中定义的所有<dependencies>全部复制粘贴到了项目B的pom.xml中。由于是“复制粘贴”，所以不是继承关系。就这个意思。
   
   另外，一个小tip，如果在B中要对A中定义的dependency覆盖，应该注意顺序，将对A的依赖卸载下面。
   
   
          这是project-B的pom.xml
          <dependencyManagement>
           <dependencies>
             <!-- 注意写到对项目A依赖的前面 -->
             <dependency>
               <groupId>some</groupId>
               <artifactId>some</artifactId>
               <version>1.0</version>
             </dependency>
        <dependency>
           <groupId>project-A</groupId>
           <artifactId>project-A</artifactId>
           <version>1.0</version>
           <scope>import</scope>
         </dependency>
         ...
       </dependencies>
         </dependencyManagement>
   **注意: 实际上不会导入依赖,有需要的在dependencies添加denpedency ga 不用写version**
   
   

## Seata

https://github.com/alibaba/spring-cloud-alibaba

https://github.com/seata/seata

seata example

https://github.com/alibaba/spring-cloud-alibaba/blob/2.2.x/spring-cloud-alibaba-examples/seata-example/readme-zh.md

TC  Transaction coordinator

TM  Transaction  Manager

RM resource Manager

![img](assets/v2-793f1e2aee1bd33f37aff7555e21d19e_1440w.jpg)

1.

```

CREATE TABLE `undo_log` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `branch_id` bigint(20) NOT NULL,
  `xid` varchar(100) NOT NULL,
  `context` varchar(128) NOT NULL,
  `rollback_info` longblob NOT NULL,
  `log_status` int(11) NOT NULL,
  `log_created` datetime NOT NULL,
  `log_modified` datetime NOT NULL,
  `ext` varchar(100) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `ux_undo_log` (`xid`,`branch_id`)
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;
```

注意,这张表应该配置到业务库里 如seata_product/seata_order



3.下载seata server并启动

​		2.2.3.RELEASE 对应 seata.version 1.3.0 

​		https://github.com/seata/seata/releases/xxx.zip

​		registry.cnf 

​			注册中心   

​	什么是注册中心?注册中心可以说是微服务架构中的”通讯录“，它记录了服务和服务地址的映射关系。在分布式架构中，服务会注册到这里，当服务需要调用其它服务时，就到这里找到服务的地址，进行调用.

​		file.cnf  

​			配置中心 

​					什么是配置中心?配置中心可以说是一个"大货仓",内部放置着各种配置文件,你可以通过自己所需进行获取配置加载到对应的客户端

​			默认 file(读本地文件 包含 conf properties yaml配置的支持)

4.

        <dependency>
            <groupId>com.alibaba.cloud</groupId>
            <artifactId>spring-cloud-starter-alibaba-seata</artifactId>
        </dependency>
        ...
        
          <dependencyManagement>
        <dependencies>
            <dependency>
                <groupId>com.alibaba.cloud</groupId>
                <artifactId>spring-cloud-alibaba-dependencies</artifactId>
                <version>2.2.3.RELEASE</version>
                <type>pom</type>
                <scope>import</scope>
            </dependency>
        </dependencies>
    </dependencyManagement>
**Product**

1. 业务的数据表(t_product)

2.

```
spring:
  application:
    name: seata-product
  cloud:
    alibaba:
      seata:
        tx-service-group: seata-product-tx-group

seata:
  enabled: true
  service:
    vgroup-mapping:
      seata-product-tx-group: default
    grouplist:
      default: 127.0.0.1:8091 # 如果尾部有: 可以加''解决特殊字符冲突
    disable-global-transaction: false
          
```

注意: 当${}作为key实际是不会被替换成真正的值的

如果想通过变量引用设置**键**，通过${}是做不到的

解决:

**通过锚点引用设置配置量的【键(key)】**

要想替换键，则需要用到锚点。锚点用符号“&”定义，并用符号“*”进行引用。
锚点有两种使用方式，一是通过“<<:”**将键值对一起引入**：

```x86asm
default-db: &default-db-config
  host: 127.0.0.1
  port: 3306

user-db:
  <<: *default-db-config
```

其实际效果如：

```yaml
default-db:
  host: 127.0.0.1
  port: 3306

user-db:
  host: 127.0.0.1
  port: 3306
```

说明：&后面的锚点名字，可自己定义，不必与配置量的键相同。

另一种是**仅引入配置的值**：

```yaml
site:
  url:
    user: &site.user www.user.net
    shop: &site.shop www.shop.net

site:
  slogan:
    *site.user: everyone is great!
    *site.shop: buy anything you want!
```

其实际效果如：

```yaml
site:
  url:
    user: www.user.net
    shop: www.shop.net

site:
  slogan:
    www.user.net: everyone is great!
    www.shop.net: buy anything you want!
```

说明：用&定义锚点时，注意锚点名前后的空格。

另一个仅引入值例子：

```markdown
color:
  - &red-color red
  - yellow
  - blue
  - *red-color
  - gray
```

其实际效果如：

```markdown
color:
  - red
  - yellow
  - blue
  - red
  - gray
```

```
&tx-group seata-product-tx-group
*tx-group: default
```

​	ok

	&tx-group ${spring.application.name}-tx-group
	*tx-group: default

​	error





**Order**

1.

```
spring:
  application:
    name: seata-order
  cloud:
    alibaba:
      seata:
        tx-service-group: seata-order-tx-group

seata:
  enabled: true
  service:
    vgroup-mapping:
      seata-order-tx-group: default
    grouplist:
      default: 127.0.0.1:8091 # 如果尾部有: 可以加''解决特殊字符冲突
    disable-global-transaction: false
```



2.

```
@GlobalTransactional
OrderServer#placeOrder(...){
restTemplate.put("http://localhost:8080/product/xxx"
}
```





## Feign

sc声明式服务调用组件

替代restTemplate



**Order端**

1.

```
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-openfeign</artifactId>
</dependency>

<dependency>
                <groupId>org.springframework.cloud</groupId>
                <artifactId>spring-cloud-dependencies</artifactId>
                <version>2020.0.3</version>
                <type>pom</type>
                <scope>import</scope>
            </dependency>
```

2.

```
ProductFeignCLient.java
/**
 * Annotation for interfaces declaring that a REST client with that interface should be
 * created (e.g. for autowiring into another component).
 *
 * an absolute URL or resolvable hostname (the protocol is optional).
 */
@FeignClient(name="seata-product",url="localhost:8080")
public interface ProductFeignCLient {

     @PutMapping("decrProduct")
     String decrProduct(@RequestParam("product_id") Long productId
            , @RequestParam("count") Integer count);
}



OrderService.java




```

 restTemplate.put("http://localhost:8080/product/decrProduct" ,uriVariable);    =>   productClient.decrProduct(10086L,100);



3.

@EnableFeignClients(basePackage="xxx")



问题1:

```
Failed to introspect Class [com.alibaba.cloud.seata.feign.SeataFeignObjectWrapper] from ClassLoader [jdk.internal.loader.ClassLoaders$AppClassLoader@2437c6dc]
```

Spring Cloud Version : Spring Cloud 2020.0.1

Spring Cloud Alibaba Version :2021.1

Spring Boot Version :2.4.2

修改Spring Cloud Alibaba Version



问题2: 注意path要跟访问的client相同

 path="/product"

**并且需要保证method也要一致**







# 分布式锁

分布式锁的目的是保证在分布式部署应用集群中，多个服务在请求同一个方法或者同一个业务操作的情况下，**对应业务逻辑只能被一台机器 上的一个进程/线程执行**，避免出现并发问题



**常用方案**

1. redis  setnx命令,原子性操作

   原子性是数据库的事务中的特性。在数据库事务的情景下，原子性指的是：一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。

   对于Redis而言，命令的原子性指的是：一个操作的不可以再分，操作要么执行，要么不执行。

   **Redis操作原子性的原因**

   Redis的操作之所以是原子性的，是因为Redis是单线程的。

   **Redis本身提供的所有API都是原子操作(原语)，Redis中的事务其实是要保证批量操作的原子性**

2. zookeeper 顺序临时节点







## redisson

https://github.com/redisson/redisson/tree/master/redisson-spring-boot-starter

Redis Java client with features of an in-memory data grid

1.

```
<dependency>
    <groupId>org.redisson</groupId>
    <artifactId>redisson-spring-boot-starter</artifactId>
    <version>3.17.5</version>
    <exclusions>
        <exclusion>
            <groupId>org.redisson</groupId>
            <artifactId>redisson-spring-data-27</artifactId>
        </exclusion>
    </exclusions>
</dependency>
<dependency>
    <groupId>org.redisson</groupId>
    <artifactId>redisson-spring-data-25</artifactId>
    <version>3.17.5</version>
</dependency>
```

注意: 3.17.5带的redisson-spring-data-27需要的sb版本是4.7

而我的sb是4.5版本,所以需要降级



>直接依赖>传递依赖(最短路径优先)
>相同路径长度
>    1. 直接依赖 覆盖原则
>        2. 传递依赖 最先声明原则奏效



2.

```
application.yml
spring:
redis:
  redisson:
    file: classpath:redisson.yaml
    
    
redisson.yaml
SingleServerConfig:
  address: "redis://127.0.0.1:6379"
```

其中config看 org.redisson.config.BaseConfig的实现类



3.

```
@RequestMapping("/lock")
    public void lock() {

        //redis中ttl默认是30s hash
        RLock lock = redissonClient.getLock("testLock");
        //the maximum time to hold the lock after it's acquisition
        //-1  lock until explicitly unlocked
//            lock.lock(5,TimeUnit.SECONDS);

        //the maximum time to wait for the lock

        if (lock.tryLock(5, TimeUnit.SECONDS)) {
            try {
                Thread.sleep(10000);
            } catch (Exception e) {
                e.printStackTrace();
            } finally {
                if (lock.isLocked()) {
                    //redis key过期
                    lock.unlock();
                }
                log.info("解锁了");
            }
        }
    }
```